{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn_crfsuite import CRF\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity recognition\n",
    "Мы будем использовать часть корпуса GMB (только часть, чтобы быстро работало). В корпусе размечены следующие тэги:\n",
    "\n",
    "Tag | Label meaning | Example Given\n",
    "--- | ------------- | -------------\n",
    "geo | Geographical Entity | London\n",
    "org | Organization | ONU\n",
    "per | Person | Bush\n",
    "gpe | Geopolitical Entity | British\n",
    "tim | Time indicator | Wednesday\n",
    "art | Artifact | Chrysler\n",
    "eve | Event | Christmas\n",
    "nat | Natural Phenomenon | Hurricane\n",
    "O | No-Label | the\n",
    "\n",
    "Тэги размечены по системе BIO. В таблице, которую мы будем использовать, есть уже все признаки по каждому слову и его контексту.\n",
    "\n",
    "Читаем и смотрим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "lemma\n",
      "next-lemma\n",
      "next-next-lemma\n",
      "next-next-pos\n",
      "next-next-shape\n",
      "next-next-word\n",
      "next-pos\n",
      "next-shape\n",
      "next-word\n",
      "pos\n",
      "prev-iob\n",
      "prev-lemma\n",
      "prev-pos\n",
      "prev-prev-iob\n",
      "prev-prev-lemma\n",
      "prev-prev-pos\n",
      "prev-prev-shape\n",
      "prev-prev-word\n",
      "prev-shape\n",
      "prev-word\n",
      "sentence_idx\n",
      "shape\n",
      "word\n",
      "tag\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n",
       "0           0  thousand         of        demonstr           NNS   \n",
       "1           1        of   demonstr            have           VBP   \n",
       "2           2  demonstr       have           march           VBN   \n",
       "3           3      have      march         through            IN   \n",
       "4           4     march    through          london           NNP   \n",
       "\n",
       "  next-next-shape next-next-word next-pos next-shape      next-word ...  \\\n",
       "0       lowercase  demonstrators       IN  lowercase             of ...   \n",
       "1       lowercase           have      NNS  lowercase  demonstrators ...   \n",
       "2       lowercase        marched      VBP  lowercase           have ...   \n",
       "3       lowercase        through      VBN  lowercase        marched ...   \n",
       "4     capitalized         London       IN  lowercase        through ...   \n",
       "\n",
       "  prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word   prev-shape  \\\n",
       "0      __start2__    __START2__        wildcard     __START2__     wildcard   \n",
       "1      __start1__    __START1__        wildcard     __START1__  capitalized   \n",
       "2        thousand           NNS     capitalized      Thousands    lowercase   \n",
       "3              of            IN       lowercase             of    lowercase   \n",
       "4        demonstr           NNS       lowercase  demonstrators    lowercase   \n",
       "\n",
       "       prev-word sentence_idx        shape           word tag  \n",
       "0     __START1__          1.0  capitalized      Thousands   O  \n",
       "1      Thousands          1.0    lowercase             of   O  \n",
       "2             of          1.0    lowercase  demonstrators   O  \n",
       "3  demonstrators          1.0    lowercase           have   O  \n",
       "4           have          1.0    lowercase        marched   O  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ner_short.csv')\n",
    "print('\\n'.join(data.columns))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        0.846915\n",
       "B-geo    0.035322\n",
       "B-tim    0.018845\n",
       "B-org    0.018625\n",
       "I-per    0.017160\n",
       "B-gpe    0.016537\n",
       "B-per    0.016404\n",
       "I-org    0.015071\n",
       "I-geo    0.007219\n",
       "I-tim    0.005703\n",
       "B-art    0.000513\n",
       "B-eve    0.000408\n",
       "I-eve    0.000325\n",
       "I-gpe    0.000311\n",
       "I-art    0.000298\n",
       "B-nat    0.000243\n",
       "I-nat    0.000101\n",
       "Name: tag, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tag.value_counts(normalize=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия\n",
    "\n",
    "Векторизуем наши фичи и объединим матрицы в одну. Сейчас я буду использовать лемму, часть речи и попробую добавить тэг предыдущего слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = CountVectorizer(token_pattern='.+')\n",
    "tag_vect = CountVectorizer(token_pattern='.+')\n",
    "\n",
    "w_vect = w_vectorizer.fit_transform(data.lemma)\n",
    "pos_vect = pos_vectorizer.fit_transform(data.pos)\n",
    "prev_tag_vect = tag_vect.fit_transform(data['prev-iob'])\n",
    "\n",
    "#X = hstack((w_vect, pos_vect))\n",
    "X = hstack((w_vect, pos_vect, prev_tag_vect))\n",
    "y = data.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на обучающую и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию с дефолтными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=42)\n",
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим качество нашей модели. Метрика - f1-score с macro усреднением - из-за дисбаланса классов (macro усреднение берет все классы с равным весом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7497133497123338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-art       0.67      0.03      0.06        61\n",
      "      B-eve       1.00      0.19      0.31        59\n",
      "      B-geo       0.72      0.92      0.81      5145\n",
      "      B-gpe       0.97      0.82      0.89      2379\n",
      "      B-nat       0.00      0.00      0.00        36\n",
      "      B-org       0.82      0.51      0.63      2767\n",
      "      B-per       0.85      0.77      0.81      2305\n",
      "      B-tim       0.98      0.73      0.84      2639\n",
      "      I-art       1.00      0.98      0.99        41\n",
      "      I-eve       1.00      0.98      0.99        47\n",
      "      I-geo       0.98      0.95      0.97      1090\n",
      "      I-gpe       1.00      0.67      0.80        52\n",
      "      I-nat       1.00      0.79      0.88        14\n",
      "      I-org       0.95      0.91      0.93      2156\n",
      "      I-per       0.94      0.99      0.97      2525\n",
      "      I-tim       0.91      0.83      0.87       828\n",
      "          O       0.99      0.99      0.99    121936\n",
      "\n",
      "avg / total       0.97      0.97      0.97    144080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "print(f1_score(y_test, y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF\n",
    "\n",
    "Для CRF нужно по-другому предобработать данные. На вход подается список предложений, где каждое предложение - это список словарей, а в словаре уже лежат признаки для каждого слова. И тут не нужно кодировать строки, подаем их как они есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = data.groupby(['sentence_idx'])\n",
    "crf_data = []\n",
    "crf_y = []\n",
    "for name, sent in sents:\n",
    "    crf_y.append(sent['tag'].tolist())\n",
    "    sent = sent[['lemma', 'pos', 'prev-iob']]\n",
    "    crf_data.append(sent.to_dict(orient='records'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(crf_data, crf_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фиттим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm=None, all_possible_states=None, all_possible_transitions=None,\n",
       "  averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n",
       "  calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=None,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = CRF()\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим скор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7924727450142887\n",
      "0.7924727450142887\n",
      "0.7924727450142887\n",
      "0.7924727450142887\n",
      "0.7924727450142887\n"
     ]
    }
   ],
   "source": [
    "crf = CRF()\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred = crf.predict(X_test)\n",
    "print(f1_score([x for y in y_test for x in y], [x for y in y_pred for x in y], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "Немного подучиться машинному обучению, подумать и к вечеру 11го января (или раньше) сделать что-нибудь, у чего скор будет лучше. Чем лучше - тем лучше :) Идеи:\n",
    "* другие модели (ансамбли и нейросети тоже можно)\n",
    "* подключить еще фичи\n",
    "* получить новые фичи из имеющихся\n",
    "* семантические вектора - попробовать использовать предобученные, обучить самим на этих данных\n",
    "* тюнинг гиперпараметров\n",
    "\n",
    "Полные данные (ок. миллиона строк) тоже лежат в репозитории. Их нужно распаковать и немного по-другому читать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ner.csv', encoding='ISO-8859-1', error_bad_lines=False)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Дополнительное задание\n",
    "В 2015 году на Диалоге проводили соревнование по сентименту. [Вот тут лежат статьи](http://www.dialog-21.ru/evaluation/2015/sentiment/) (и ссылка на материалы тоже лежит). Попробуйте:\n",
    "* скачать [этот файл](https://drive.google.com/file/d/0B7y8Oyhu03y_YV9ZU0cwZGVtenM/view)\n",
    "* распарсить его (`lxml.etree`, с этим могу помочь)\n",
    "* предсказать по тексту сентимент (атрибут `sentiment`) по аспектам, которые указаны в `<categories>` - смотрите по метрике f-score с макро-усреднением - на соревновании это было задание Д (а основное - выделить аспекты)\n",
    "* подумать, как это можно улучшить\n",
    "* возможно, стоит брать не текст целиком а уметь находить конкретные аспекты и их сентимент в тексте -> задача выделения аспектов (сложная)\n",
    "* вдохновляться статьями можно, смотреть скоры в обзорной статье и в табличке на гуглдиске можно"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
